{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559022b2-d0df-48bc-841c-7f6985b0c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch nltk rouge_score wandb pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb85c2-3d8d-4591-8c4c-55fe2b8fa2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "# from framework.utilities.models import inference    \n",
    "from framework.utilities.metrics import calculate_bleu, calculate_rouge\n",
    "import fastText\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "import json\n",
    "import wandb\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d3fd4-70ab-4ab7-8655-026494ee67d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from power_monitoring.monitor import HWMonitor\n",
    "import threading\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c35647b-894f-4627-b57d-330eba72e97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ccdead-eb3c-48fb-ad8e-3a7e2afc78b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_outputs[\"gsm8k\"] = [{\"input_text\": None, \"7b\": None, \"tiny\": None, \"13b\": None} for _ in range(3000)]\n",
    "# len(combined_outputs[\"gsm8k\"])\n",
    "\n",
    "# with open('data/input_output_train', 'wb') as f:\n",
    "#     pickle.dump(combined_outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d038580-deb8-4ade-a503-f54e18e9c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open('data/input_output_train', 'rb') as file:\n",
    "    combined_outputs = pickle.load(file)\n",
    "\n",
    "df_combined_outputs = pd.DataFrame(combined_outputs)\n",
    "df_combined_outputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f54d5-e9f2-4812-b50b-d7045e7c14f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_outputs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c195e20c-b4f2-4fe2-ab6c-29a84e092b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_outputs = df_combined_outputs.sample(frac=1, random_state=365).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22851f3c-bc40-4374-9a2f-04633425d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_outputs[\"cnn_dailymail\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229256f8-e265-4afd-a42c-059924698cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm(T, V, h_tilde, c):\n",
    "    l_predictor, s_predictor = None, None\n",
    "    Q = 0.0 \n",
    "    results = []\n",
    "\n",
    "    l_loss_all = []\n",
    "    s_loss_all = []\n",
    "    acc_all = []\n",
    "    energy_all = []\n",
    "    total_acc = 0\n",
    "\n",
    "    for t in range(1, T+1):\n",
    "        print(f\"Index: {t}\")\n",
    "        t_i = df_combined_outputs[\"wmt14\"][t][\"input_text\"]\n",
    "        t_e = df_combined_outputs[\"wmt14\"][t][\"label\"]\n",
    "\n",
    "        p_t = min(1, c / t**(1/3))\n",
    "        X_t = Bernoulli(p_t)\n",
    "\n",
    "        l_loss, s_loss = None, None\n",
    "        l_predicted_acc, s_predicted_acc = None, None\n",
    "\n",
    "        g_large = 5.5\n",
    "        g_small = 1\n",
    "\n",
    "        # CNN Dailymail: [750.2854566666667, 142.08939366666667]\n",
    "\n",
    "        if X_t == 1:\n",
    "            t_c = getResults(t, t_i, t_e)\n",
    "            l_predictor, s_predictor, l_loss, s_loss = sgdStep(l_predictor, s_predictor, t_c)\n",
    "            l_predicted_acc, s_predicted_acc = predict(t_i, l_predictor, s_predictor)\n",
    "            l_predictor, s_predictor = checkpoint(l_predictor, s_predictor, t, p_t)\n",
    "        else:\n",
    "            l_predicted_acc, s_predicted_acc = predict(t_i, l_predictor, s_predictor)\n",
    "\n",
    "        t_c = getResults(t, t_i, t_e)\n",
    "        cost_large = V * g_large + Q * (h_tilde - l_predicted_acc)\n",
    "        cost_small = V * g_small + Q * (h_tilde - s_predicted_acc)\n",
    "\n",
    "        print('Q=', Q)\n",
    "        if cost_large < cost_small:\n",
    "            x_t = 'large'\n",
    "            selected_acc = l_predicted_acc\n",
    "            selected_energy = g_large\n",
    "            print('selected large')\n",
    "        else:\n",
    "            x_t = 'small'\n",
    "            selected_acc = s_predicted_acc\n",
    "            selected_energy = g_small\n",
    "            print('selected small')\n",
    "\n",
    "        output, t_acc = querySelectedModel(t, t_e, x_t)\n",
    "        total_acc += t_acc\n",
    "\n",
    "        if l_loss is not None:\n",
    "            l_loss_all.append(l_loss)\n",
    "        if s_loss is not None:\n",
    "            s_loss_all.append(s_loss)\n",
    "        acc_all.append(t_acc)\n",
    "        energy_all.append(selected_energy)\n",
    "\n",
    "        avg_l_loss = np.mean(l_loss_all)\n",
    "        avg_s_loss = np.mean(s_loss_all)\n",
    "        avg_acc = np.mean(acc_all)\n",
    "        avg_energy = np.mean(energy_all)\n",
    "\n",
    "        Q = max(0.0, Q + h_tilde - t_acc)\n",
    "        log(results, t, t_i, output, avg_acc, avg_energy, x_t, avg_l_loss, avg_s_loss)\n",
    "\n",
    "    return results, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f71fb4f-e6c0-495f-bca0-4eaa1520d163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(results, t, t_i, output, t_acc, t_energy, x_t, l_loss, s_loss):\n",
    "    results.append({\n",
    "        'iteration': t,\n",
    "        'input': t_i,\n",
    "        'best_output': output,\n",
    "        'chosen_models_avg_accuracy': t_acc,\n",
    "        'chosen_models_avg_energy': t_energy,\n",
    "        'chosen_model': x_t,\n",
    "        'l_loss': l_loss,\n",
    "        's_loss': s_loss\n",
    "    })\n",
    "    log_data = {\n",
    "        'iteration': t,\n",
    "        'chosen_model_accuracy': t_acc,\n",
    "        'chosen_model_energy': t_energy\n",
    "    }\n",
    "        \n",
    "    if l_loss is not None: log_data['l_loss'] = l_loss\n",
    "    if s_loss is not None: log_data['s_loss'] = s_loss\n",
    "        \n",
    "    wandb.log(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3f4257-4e9b-49ba-ad82-312b64d73ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bernoulli(p_t):\n",
    "    return random.random() < p_t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab290fb6-de85-40ca-bc29-9bf3654a9f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def querySelectedModel(t, t_e, x_t):\n",
    "    if x_t == \"large\":\n",
    "        output = df_combined_outputs[\"wmt14\"][t][\"13b\"]\n",
    "    else:\n",
    "        output =  df_combined_outputs[\"wmt14\"][t][\"tiny\"]\n",
    "        \n",
    "    return output, calculate_bleu(output, t_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f84c0-4552-4f62-bf95-19cddaed2bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def queryBest(t, t_i, t_e, l_predictor, s_predictor):\n",
    "#     l_acc, s_acc = predict(t_i, l_predictor, s_predictor)\n",
    "#     if l_acc > s_acc: # use larger model, e.g. 7b llama\n",
    "#         output = df_combined_outputs[\"wmt14\"][t][\"13b\"]\n",
    "#     else: # use smaller model for all other cases, e.g. 3b llama\n",
    "#         output =  df_combined_outputs[\"wmt14\"][t][\"tiny\"]\n",
    "\n",
    "#     return output, calculate_bleu(output, t_e)\n",
    "#     # return output, calculate_rouge(output, t_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce7f4c-3666-4468-ad22-834e0530dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, window_size):\n",
    "    data_list = list(data)  \n",
    "    if len(data_list) == 0:\n",
    "        return 0.0 \n",
    "    if len(data_list) < window_size:\n",
    "        return sum(data_list) / len(data_list)\n",
    "    return sum(data_list[-window_size:]) / window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d6079-a233-4862-8e7f-918c12cb4efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_loss(predicted_accuracy, true_accuracy):\n",
    "    predicted_tensor = torch.tensor([predicted_accuracy], dtype=torch.float32)\n",
    "    true_tensor = torch.tensor([true_accuracy], dtype=torch.float32)\n",
    "\n",
    "    loss = F.mse_loss(predicted_tensor, true_tensor)\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df16dc77-e1db-4a51-a8bc-c1f76e85c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgdStep(large_model_predictor, small_model_predictor, t_c):\n",
    "    predicted_large_label, predicted_small_label = 0, 0\n",
    "    if large_model_predictor and small_model_predictor != None: \n",
    "        predicted_large_label = large_model_predictor.predict(t_c['input_text'])[0][0].replace(\"__label__\", \"\")\n",
    "        predicted_small_label = small_model_predictor.predict(t_c['input_text'])[0][0].replace(\"__label__\", \"\")\n",
    "    \n",
    "    with open(\"fasttext_large.txt\", \"w\") as f:\n",
    "        f.write(f\"__label__{t_c['large_model_accuracy']} {t_c['input_text']}\\n\")\n",
    "    with open(\"fasttext_small.txt\", \"w\") as f:\n",
    "        f.write(f\"__label__{t_c['small_model_accuracy']} {t_c['input_text']}\\n\")\n",
    "\n",
    "    if os.path.exists(\"large_predictor.bin\"):\n",
    "        large_model_predictor = fastText.train_supervised(input=\"fasttext_large.txt\", epoch=1, lr=1.0, wordNgrams=2, inputModel=\"large_predictor.bin\")\n",
    "    else:\n",
    "        large_model_predictor = fastText.train_supervised(input=\"fasttext_large.txt\", epoch=1, lr=1.0, wordNgrams=2)\n",
    "\n",
    "    if os.path.exists(\"small_predictor.bin\"):\n",
    "        small_model_predictor = fastText.train_supervised(input=\"fasttext_small.txt\", epoch=1, lr=1.0, wordNgrams=2, inputModel=\"small_predictor.bin\")\n",
    "    else:\n",
    "        small_model_predictor = fastText.train_supervised(input=\"fasttext_small.txt\", epoch=1, lr=1.0, wordNgrams=2)\n",
    "\n",
    "    large_model_loss = compute_mse_loss(float(predicted_large_label), t_c['large_model_accuracy'])\n",
    "    small_model_loss = compute_mse_loss(float(predicted_small_label), t_c['small_model_accuracy'])\n",
    "\n",
    "    # print(f\"Large Stats(predicted, actual): {predicted_large_label}, {t_c['large_model_accuracy']}\")\n",
    "    # print(f\"Mean Square Error: {large_model_loss}\")\n",
    "    # print(f\"Small Stats(predicted, actual): {predicted_small_label}, {t_c['small_model_accuracy']}\")\n",
    "    # print(f\"Mean Square Error: {large_model_loss}\")\n",
    "\n",
    "    return large_model_predictor, small_model_predictor, large_model_loss, small_model_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0e73e-7a06-4071-9175-adbd53710be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, l_predictor, s_predictor):\n",
    "    l_predicted_label = l_predictor.predict(text)[0][0]\n",
    "    l_predicted_accuracy = int(round(float(l_predicted_label.replace('__label__', ''))))\n",
    "\n",
    "    s_predicted_label = s_predictor.predict(text)[0][0]\n",
    "    s_predicted_accuracy = int(round(float(s_predicted_label.replace('__label__', ''))))\n",
    "\n",
    "    return l_predicted_accuracy, s_predicted_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d56eb4-a502-4583-961d-7b7c39eccd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResults(t, t_i, t_e):\n",
    "    s_output = df_combined_outputs[\"wmt14\"][t][\"tiny\"]\n",
    "    l_output = df_combined_outputs[\"wmt14\"][t][\"13b\"]\n",
    "    \n",
    "    # WMT14\n",
    "    s_acc = calculate_bleu(s_output, t_e)\n",
    "    l_acc = calculate_bleu(l_output, t_e)\n",
    "\n",
    "    # CNN Dailymail\n",
    "    # s_acc = calculate_rouge(s_output, t_e)['rouge1']\n",
    "    # l_acc = calculate_rouge(l_output, t_e)['rouge1']\n",
    "\n",
    "    return {\n",
    "        'input_text': t_i,\n",
    "        'large_model_accuracy': l_acc,\n",
    "        'small_model_accuracy': s_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657f810-6521-4073-ab4d-bb1e7dee9648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(l_predictor, s_predictor, t, p_t):\n",
    "    if p_t > 0.5:\n",
    "        if t % int(t**(1/3)) == 0:\n",
    "            print(\"Checkpoint\")\n",
    "            l_predictor, s_predictor = save_models(l_predictor, s_predictor)\n",
    "    else:\n",
    "        if t % max(1, int(1 / t**(1/3))) == 0:\n",
    "            print(\"Checkpoint\")\n",
    "            l_predictor, s_predictor = save_models(l_predictor, s_predictor)\n",
    "\n",
    "    return l_predictor, s_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534194e3-df63-47f4-8aac-382813becd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(l_predictor, s_predictor):\n",
    "    if l_predictor:\n",
    "        l_predictor.save_model(\"large_predictor.bin\")\n",
    "    if s_predictor:\n",
    "        s_predictor.save_model(\"small_predictor.bin\")\n",
    "\n",
    "    return l_predictor, s_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404d0a7-f482-44d8-ac62-3d34c5b59f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_record(T, V, h_tilde, c, run_name):\n",
    "    wandb.init(project=\"classifier\", name=run_name)\n",
    "    # stop_event = threading.Event()\n",
    "    # hw_monitor = HWMonitor(monitoring_freq=1.0, stop_event=threading.Event())\n",
    "    # hw_monitor.start()\n",
    "    \n",
    "    results, total_acc = algorithm(T, V, h_tilde, c)\n",
    "\n",
    "    # stop_event.set() \n",
    "    # hw_monitor.join()\n",
    "    wandb.finish()\n",
    "\n",
    "    return results, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704838e6-8ec2-49c0-b884-838f5e1e2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_tilde_config = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8fece-4041-47ac-8ee4-9b0dd0d67f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = [1, 3, 5, 10]\n",
    "V = .1\n",
    "total_acc_list = []\n",
    "\n",
    "for c in c_values:\n",
    "    run_name = f'V{V}_c{c}_h{h_tilde_config}_SEED135'  \n",
    "    results, total_acc = run_and_record(2999, V, h_tilde_config, c, run_name)\n",
    "    print(f\"AVERAGE ACCURACY for c={c}: {total_acc / 3000}\")\n",
    "    total_acc_list.append(total_acc)\n",
    "    \n",
    "    filename = f'experiments/fasttext/{run_name}.json'\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b231c9a2-f885-413a-aa23-586099218e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_values = [.01, 0.1, 1, 10, 100]\n",
    "\n",
    "total_acc_list = []\n",
    "\n",
    "for V in V_values:\n",
    "    run_name = f'fasttext_wmt14_results_V{V}_h{h_tilde_config}'  \n",
    "    results, total_acc = run_and_record(2999, V, h_tilde_config, 3, run_name)\n",
    "    print(f\"AVERAGE ACCURACY: {total_acc / 3000}\")\n",
    "    total_acc_list.append(total_acc)\n",
    "    filename = f'experiments/fasttext/{run_name}.json'\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ce4e8-cca4-4d3a-b082-11a9651fb349",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d2e30c-bcaa-4844-b97c-df97df4412a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_tilde_config = .55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dff5b60-a3bc-41b0-b899-a1c8c5cb7c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_values = [10000, 100000]   # 0.01, 0.1, 1, 10, 100, 1000,\n",
    "\n",
    "for V in V_values:\n",
    "    run_name = f'fasttext_wmt14_results_V{V}_h{h_tilde_config}'  \n",
    "    results = run_and_record(2999, V, h_tilde_config, 5, run_name)\n",
    "    filename = f'experiments/fasttext/{run_name}.json'\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200e54a1-ff42-40b0-acfb-36fdfbcc405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "num_results = len(data)\n",
    "\n",
    "for result in data:\n",
    "    total += result['chosen_model_accuracy']\n",
    "\n",
    "total / num_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885488a4-662f-49fb-9aa9-d4bec5f66cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cnndailymail_results.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3296c62-d69f-4531-9c20-24935ba4b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge1_total = 0\n",
    "rouge2_total = 0\n",
    "rougeL_total = 0\n",
    "\n",
    "num_results = len(data)\n",
    "\n",
    "for result in data:\n",
    "    rouge1_total += result['chosen_model_accuracy']['rouge1']\n",
    "    rouge2_total += result['chosen_model_accuracy']['rouge2']\n",
    "    rougeL_total += result['chosen_model_accuracy']['rougeL']\n",
    "\n",
    "rouge1_avg = rouge1_total / num_results\n",
    "rouge2_avg = rouge2_total / num_results\n",
    "rougeL_avg = rougeL_total / num_results\n",
    "\n",
    "print(f\"Average ROUGE-1: {rouge1_avg}\")\n",
    "print(f\"Average ROUGE-2: {rouge2_avg}\")\n",
    "print(f\"Average ROUGE-L: {rougeL_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd8cbae-a976-4c3a-9632-c336cb23c271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
