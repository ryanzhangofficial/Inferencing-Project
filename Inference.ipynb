{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391b110-96c2-45d9-8779-085ac21bb7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets sentence_transformers matplotlib nltk accelerate wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43999e6c-236b-4e82-b38c-6e09467f0b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4521b4ea-f7bf-4a5c-9613-baed1a083353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from power_monitoring.monitor import HWMonitor\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe3d7cf-7a16-4a31-944a-47a72515b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1ddf2-e156-49e2-81c8-2a0f2ce9f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249643e6-bc41-4260-85e7-467000381574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72070d98-81c5-47af-bf3e-99fc77698ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd9d364-a98f-4d3a-9582-e3e56687978e",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2db1a4-aad6-40d7-8927-79f8b0a9bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "llama7b_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llama7b = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                               torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf103e-2b3f-40ed-9eaa-5f9ef1a0b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "\n",
    "tinyllama_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tinyllama = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                 torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a00c87-763b-45ff-ba95-e4a60f006ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "\n",
    "llama13b_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llama13b = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                               torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2141af-b150-4eeb-ae89-646b3af73db7",
   "metadata": {},
   "source": [
    "## Perform Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba38204-74c6-439a-b38f-0a99fee523a3",
   "metadata": {},
   "source": [
    "WMT14(Translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4186003-a5d5-47c2-b12d-65f4615a7f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wmt14_dataset = load_dataset('wmt14', 'de-en', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f5b736-93a7-4f15-bfaa-e2347dd16d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = wmt14_dataset[2]['translation']['de']  \n",
    "input_prompt = \"Translate the sentence from German to English: \\n\\n\" + input_text + \"\\n\\n Write the translation here: \"\n",
    "\n",
    "inputs = llama7b_tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a0c3a6-36d4-4aa5-9144-6a0d7de09531",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72cbeec-4cdf-4143-a5fe-aa17faf6be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_ids = llama7b.generate(inputs['input_ids'])\n",
    "\n",
    "output_text = llama7b_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "answer_prefix = \"Write the translation here: \"\n",
    "if answer_prefix in output_text:\n",
    "    cleaned_output = output_text.split(answer_prefix)[-1].strip()\n",
    "else:\n",
    "    cleaned_output = output_text.strip()\n",
    "\n",
    "first_sentence = cleaned_output.split('.')[0] + '.' if '.' in cleaned_output else cleaned_output\n",
    "print(first_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a11fb-a843-40fc-ae03-a6f8aa878ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {\n",
    "    \"wmt14\": [],\n",
    "    \"cnn_dailymail\": [],\n",
    "    \"gsm8k\": []\n",
    "}\n",
    "\n",
    "for i in range(3000):\n",
    "    input_text = wmt14_dataset[i]['translation']['de']\n",
    "    outputs[\"wmt14\"].append({\n",
    "        \"input_text\": input_text,\n",
    "        \"tiny\": None,\n",
    "        \"7b\": None,\n",
    "        \"13b\": None\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146da8cb-3d13-4596-9096-889e25b15b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"experiments/input_output_train\", 'wb') as f:\n",
    "    pickle.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a10ad80-a1c1-4c8b-ad0b-d56dd7b5aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"experiments/input_output_train\", 'rb') as f:\n",
    "    outputs = pickle.load(f)\n",
    "print(outputs['wmt14'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34004146-ec10-472a-b557-36811673b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wmt14(model, tokenizer, dataset, output_file, num_samples, dict_type):\n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        input_text = dataset[i]['translation']['de']\n",
    "        input_prompt = \"Translate the sentence from German to English: \\n\\n\" + input_text + \"\\n\\n Write the translation here: \"\n",
    "\n",
    "        inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(inputs['input_ids'])\n",
    "\n",
    "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        answer_prefix = \"Write the translation here: \"\n",
    "        if answer_prefix in output_text:\n",
    "            cleaned_output = output_text.split(answer_prefix)[-1].strip()\n",
    "        else:\n",
    "            cleaned_output = output_text.strip()\n",
    "\n",
    "        first_sentence = cleaned_output.split('.')[0] + '.' if '.' in cleaned_output else cleaned_output\n",
    "        print(f\"{dict_type} | CURRENT IDX: {i}\")\n",
    "\n",
    "        outputs[\"wmt14\"][i][dict_type] = first_sentence\n",
    "\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(outputs, f)\n",
    "\n",
    "    print(f\"Generated {num_samples} sentences for {dict_type} and saved to {output_file}.\")\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbe477b-b492-4475-b3d4-385dd25dc6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"classifier\")\n",
    "stop_event = threading.Event()\n",
    "hw_monitor = HWMonitor(monitoring_freq=1.0, stop_event=threading.Event())\n",
    "hw_monitor.start()\n",
    "\n",
    "generated_samples = generate_wmt14(llama7b, llama7b_tokenizer, wmt14_dataset, \"experiments/input_output_train\", 3000, \"7b\")\n",
    "\n",
    "stop_event.set() \n",
    "hw_monitor.join()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0516bd-2783-483b-8ba1-3063c2f5a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"classifier\")\n",
    "stop_event = threading.Event()\n",
    "hw_monitor = HWMonitor(monitoring_freq=1.0, stop_event=threading.Event())\n",
    "hw_monitor.start()\n",
    "\n",
    "generated_samples = generate_wmt14(llama13b, llama13b_tokenizer, wmt14_dataset, \"experiments/input_output_train\", 3000, \"13b\")\n",
    "\n",
    "stop_event.set() \n",
    "hw_monitor.join()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021eaf10-1c17-4357-bdcb-6c7b2b1805c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"classifier\")\n",
    "stop_event = threading.Event()\n",
    "hw_monitor = HWMonitor(monitoring_freq=1.0, stop_event=threading.Event())\n",
    "hw_monitor.start()\n",
    "\n",
    "generated_samples = generate_wmt14(tinyllama, tinyllama_tokenizer, wmt14_dataset, \"experiments/input_output_train\", 3000, \"tiny\")\n",
    "\n",
    "stop_event.set() \n",
    "hw_monitor.join()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bed935f-f35e-4e38-ac53-e5d62e98f649",
   "metadata": {},
   "source": [
    "CNN Dailymail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8a7a02-d873-4514-95b3-73d82813e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cnndailymail_dataset = load_dataset('abisee/cnn_dailymail', '2.0.0', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af74466-059f-4205-9a11-3ccb2b0087f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = cnn_dailymail_dataset[100]['article'] \n",
    "input_prompt = \"Summarize the following text in under 50 words: \\n\\n\" + input_text + \"\\n\\n Write the summary here: \"\n",
    "\n",
    "inputs = tinyllama_tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed7a94-c773-4d71-801f-c437f322d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tinyllama.generate(inputs['input_ids'])\n",
    "\n",
    "output_text = tinyllama_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input: {input_prompt}\")\n",
    "\n",
    "summary_prefix = \"Write the summary here: \"\n",
    "if summary_prefix in output_text:\n",
    "    cleaned_output = output_text.split(summary_prefix)[-1].strip()\n",
    "else:\n",
    "    cleaned_output = output_text.strip()\n",
    "\n",
    "print(cleaned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07012598-fed7-4205-b5f0-af49a8e02683",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"experiments/input_output_train\", 'rb') as f:\n",
    "    outputs = pickle.load(f)\n",
    "print(outputs['wmt14'])\n",
    "\n",
    "for i in range(3000):\n",
    "    input_text = cnndailymail_dataset[i]['article']\n",
    "    outputs[\"cnn_dailymail\"].append({\n",
    "        \"input_text\": input_text,\n",
    "        \"tiny\": None,\n",
    "        \"7b\": None,\n",
    "        \"13b\": None\n",
    "    })\n",
    "    \n",
    "with open(\"experiments/input_output_train\", 'wb') as f:\n",
    "    pickle.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6511f25e-7a6b-4aba-af41-39a87ef4797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cnndailymail(model, tokenizer, dataset, output_file, num_samples, dict_type):\n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        input_text = dataset[i]['article'] \n",
    "        input_prompt = \"Summarize the following text in under 50 words: \\n\\n\" + input_text + \"\\n\\n Write the summary here: \"\n",
    "\n",
    "        inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "\n",
    "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        summary_prefix = \"Write the summary here: \"\n",
    "        if summary_prefix in output_text:\n",
    "            cleaned_output = output_text.split(summary_prefix)[-1].strip()\n",
    "        else:\n",
    "            cleaned_output = output_text.strip()\n",
    "\n",
    "        first_sentence = cleaned_output.split('.')[0] + '.' if '.' in cleaned_output else cleaned_output\n",
    "        print(f\"{dict_type} | CURRENT IDX: {i}\")\n",
    "\n",
    "        outputs[\"cnn_dailymail\"][i][dict_type] = first_sentence\n",
    "\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(outputs, f)\n",
    "\n",
    "    print(f\"Generated {num_samples} sentences for {dict_type} and saved to {output_file}.\")\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43124f14-206a-4323-81b2-0348e06f196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"classifier\")\n",
    "stop_event = threading.Event()\n",
    "hw_monitor = HWMonitor(monitoring_freq=1.0, stop_event=threading.Event())\n",
    "hw_monitor.start()\n",
    "\n",
    "generated_samples = generate_cnndailymail(llama7b, llama7b_tokenizer, cnndailymail_dataset, \"experiments/input_output_train\", 3000, \"7b\")\n",
    "\n",
    "stop_event.set() \n",
    "hw_monitor.join()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c30c8-22d7-400a-867a-ea3981e02bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"classifier\")\n",
    "stop_event = threading.Event()\n",
    "hw_monitor = HWMonitor(monitoring_freq=1.0, stop_event=threading.Event())\n",
    "hw_monitor.start()\n",
    "\n",
    "generated_samples = generate_cnndailymail(llama13b, llama13b_tokenizer, cnndailymail_dataset, \"experiments/input_output_train\", 3000, \"13b\")\n",
    "\n",
    "stop_event.set() \n",
    "hw_monitor.join()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eb6534-05db-40fa-af5e-1be6b63021ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"classifier\")\n",
    "stop_event = threading.Event()\n",
    "hw_monitor = HWMonitor(monitoring_freq=1.0, stop_event=threading.Event())\n",
    "hw_monitor.start()\n",
    "\n",
    "generated_samples = generate_cnndailymail(tinyllama, tinyllama_tokenizer, cnndailymail_dataset, \"experiments/input_output_train\", 3000, \"tiny\")\n",
    "\n",
    "stop_event.set() \n",
    "hw_monitor.join()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fe0881-e772-4881-9b6a-be62686e693c",
   "metadata": {},
   "source": [
    "GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e281f92-9a1f-410e-9139-433b602585b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "gsm8k_dataset = load_dataset(\"openai/gsm8k\", \"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586ff52-1442-49d7-88b1-9288d93f7897",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"experiments/input_output_train\", 'rb') as f:\n",
    "    outputs = pickle.load(f)\n",
    "# print(outputs['wmt14'])\n",
    "\n",
    "for i in range(3000):\n",
    "    input_text = gsm8k_dataset['train'][i]['question']\n",
    "    outputs[\"gsm8k\"].append({\n",
    "        \"input_text\": input_text,\n",
    "        \"tiny\": None,\n",
    "        \"7b\": None,\n",
    "        \"13b\": None\n",
    "    })\n",
    "    \n",
    "with open(\"experiments/input_output_train\", 'wb') as f:\n",
    "    pickle.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb91620-2fa2-4225-b39a-1af533c2bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gsm8k(model, tokenizer, dataset, output_file, num_samples, dict_type):\n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        input_question = dataset[\"train\"][i]['question']\n",
    "        input_prompt = \"Solve the following math problem step by step: \\n\\n\" + input_question + \"\\n\\n Provide your solution here: \"\n",
    "\n",
    "        inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(inputs['input_ids'], max_new_tokens=150)\n",
    "\n",
    "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        solution_prefix = \"Provide your solution here: \"\n",
    "        if solution_prefix in output_text:\n",
    "            cleaned_output = output_text.split(solution_prefix)[-1].strip()\n",
    "        else:\n",
    "            cleaned_output = output_text.strip()\n",
    "\n",
    "        print(f\"{dict_type} | CURRENT IDX: {i}\")\n",
    "\n",
    "        outputs[\"gsm8k\"][i][dict_type] = cleaned_output\n",
    "\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(outputs, f)\n",
    "\n",
    "    print(f\"Generated {num_samples} solutions for {dict_type} and saved to {output_file}.\")\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2325dabf-54ad-43c5-8853-daed9df32197",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"classifier\")\n",
    "stop_event = threading.Event()\n",
    "hw_monitor = HWMonitor(monitoring_freq=1.0, stop_event=threading.Event())\n",
    "hw_monitor.start()\n",
    "\n",
    "generated_samples = generate_gsm8k(llama7b, llama7b_tokenizer, gsm8k_dataset, \"experiments/input_output_train\", 3000, \"7b\")\n",
    "\n",
    "stop_event.set() \n",
    "hw_monitor.join()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be159d89-e2ae-4f93-85b9-1b63e17aa7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0de97-f0b8-4c0d-a102-598e103bcf11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
